{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "import queue\n",
    "import numpy as np\n",
    "import gensim\n",
    "from collections import defaultdict\n",
    "from functools import reduce, partial\n",
    "from scipy.spatial.distance import euclidean\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora, models\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from nltk import tokenize\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn import manifold\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from source.Topic.Utils import *\n",
    "from source.Topic.Model import *\n",
    "import re\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "meta = pd.read_csv(\"kaffee_reviews.csv\")\n",
    "meta.drop(columns=[\"index\"], inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing raw texts ...\n",
      "Preprocessing raw texts. Done!\n"
     ]
    }
   ],
   "source": [
    "method = \"BERT_AE\" # \"LDA_BERT\"\n",
    "samp_size = len(meta.review)\n",
    "ntopic = 10\n",
    "\n",
    "rws = meta.review\n",
    "#rws = pd.Series(sentences)\n",
    "sentences, token_lists, idx_in = preprocess(rws, samp_size=samp_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"distiluse-base-multilingual-cased\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "words, words_counts = np.unique(np.concatenate(token_lists), return_counts=True)\n",
    "embeddings = model.encode(words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters: 44\n",
      "Estimated number of noise points: 762\n"
     ]
    }
   ],
   "source": [
    "def synonyme(unique_words_count, embeddings, eps=0.3, min_samples=2):\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples).fit(embeddings)\n",
    "    labels = db.labels_\n",
    "\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise_ = list(labels).count(-1)\n",
    "\n",
    "    print('Estimated number of clusters: %d' % n_clusters_)\n",
    "    print('Estimated number of noise points: %d' % n_noise_)\n",
    "    #[np.where(labels == i)[0] for i in range(n_clusters_)]\n",
    "    count = 0\n",
    "    syn = {}\n",
    "    syn_i = {}\n",
    "    for word_idx in np.where(labels == -1)[0]:\n",
    "        syn[word_idx] = count\n",
    "        syn_i[count] = word_idx\n",
    "        count += 1\n",
    "    for cluster in [np.where(labels == i)[0] for i in range(n_clusters_)]:\n",
    "        # print([words[j] for j in cluster])\n",
    "        for word_idx in cluster:\n",
    "            syn[word_idx] = count\n",
    "        syn_i[count] = cluster[unique_words_count[cluster].argmax()]\n",
    "        count += 1\n",
    "    return syn, syn_i, count\n",
    "\n",
    "syn, syn_i, n_syn = synonyme(words_counts, embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "mat = np.zeros((n_syn, n_syn))\n",
    "for k, token_list in enumerate(token_lists):\n",
    "    syn_tokens = np.unique([syn[np.where(words==i)[0][0]] for i in token_list])\n",
    "    token_combinations = np.array(np.meshgrid(syn_tokens, syn_tokens)).T.reshape(-1, 2)\n",
    "    for i, j in token_combinations:\n",
    "        if i == j:\n",
    "            mat[i][j] += 1 # len(syn_tokens) - 1\n",
    "        else:\n",
    "            mat[i][j] += 1\n",
    "\n",
    "mat_p = np.zeros((n_syn, n_syn))\n",
    "for i in range(n_syn):\n",
    "    if mat[i][i] != 0:\n",
    "        mat_p[i] = mat[i] / mat[i][i]\n",
    "        # mat_p[i][i] = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "[('Kaffee', 100.0),\n ('Geschmack', 28.37),\n ('Aroma', 14.54),\n ('preis', 14.54),\n ('Bio', 13.48),\n ('Leck', 11.35),\n ('Produkt', 9.93),\n ('Bohne', 9.57),\n ('Sorte', 9.57),\n ('Qualität', 8.16)]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kaffee = 239, syn[Kaffee] = 778\n",
    "# Geschmack = 178, syn[Geschmack] = 775\n",
    "# argmax 775\n",
    "# q = syn[np.where(words == \"Bewertung\")[0][0]]\n",
    "q = 778\n",
    "[(words[syn_i[i]], np.round(mat_p[q][i] * 100, decimals=2)) for i in np.argsort(mat_p[q])[::-1]][:10]\n",
    "# [np.round(sum(mat_p[i]), decimals=3) for i in range(mat_p.shape[0])]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "array([778, 775, 804,  62, 764, 344, 400, 265,  70, 413], dtype=int64)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort([sum(mat[i]) for i in range(n_syn)])[::-1][:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "[('Geschmack', 100.0, 116.0),\n ('preis', 41.18, 21.0),\n ('Cafe', 40.74, 11.0),\n ('Bio', 31.48, 17.0),\n ('Aroma', 28.57, 16.0),\n ('Kaffee', 28.37, 80.0),\n ('Leck', 27.66, 13.0)]"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 775\n",
    "threshold = 10\n",
    "[(words[syn_i[i]], np.round(mat_p[:, p][i] * 100, decimals=2), mat[:,p][i]) for i in np.argsort(mat_p[:, p])[::-1] if mat[:, p][i] > threshold][:10]\n",
    "#mat_p[:, 775]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Kaffee', 100.0), ('Geschmack', 28.37), ('Aroma', 14.54), ('preis', 14.54), ('Bio', 13.48), ('Leck', 11.35), ('Produkt', 9.93), ('Bohne', 9.57), ('Sorte', 9.57), ('Qualität', 8.16)]\n",
      "[('Geschmack', 100.0), ('Kaffee', 68.97), ('preis', 18.1), ('Bio', 14.66), ('Aroma', 13.79), ('Leck', 11.21), ('Cafe', 9.48), ('fairtrade', 8.62), ('Produkt', 8.62), ('Sorte', 8.62)]\n",
      "[('preis', 100.0), ('Kaffee', 80.39), ('Geschmack', 41.18), ('Sorte', 21.57), ('Aroma', 17.65), ('Qualität', 15.69), ('Cafe', 15.69), ('Bio', 15.69), ('Leck', 15.69), ('Bohne', 13.73)]\n",
      "[('Bio', 100.0), ('Kaffee', 70.37), ('Geschmack', 31.48), ('Aroma', 18.52), ('Produkt', 16.67), ('Qualität', 14.81), ('preis', 14.81), ('fairtrade', 12.96), ('Sorte', 9.26), ('Magen', 9.26)]\n",
      "[('Aroma', 100.0), ('Kaffee', 73.21), ('Geschmack', 28.57), ('Bio', 17.86), ('preis', 16.07), ('Bohne', 14.29), ('Cafe', 12.5), ('Produkt', 10.71), ('Duft', 10.71), ('kaffeevollautomat', 8.93)]\n",
      "[('Produkt', 100.0), ('Kaffee', 75.68), ('Geschmack', 27.03), ('Bio', 24.32), ('Aroma', 16.22), ('Stern', 13.51), ('Tag', 10.81), ('preis', 10.81), ('Preis', 8.11), ('Sinn', 8.11)]\n",
      "[('Sorte', 100.0), ('Kaffee', 75.0), ('preis', 30.56), ('Geschmack', 27.78), ('Bio', 13.89), ('Qualität', 13.89), ('Firma', 11.11), ('Bohne', 11.11), ('Latte', 11.11), ('fairtrade', 11.11)]\n",
      "[('Leck', 100.0), ('Kaffee', 68.09), ('Geschmack', 27.66), ('preis', 17.02), ('Milch', 14.89), ('Bohne', 14.89), ('Bio', 10.64), ('zuckern', 10.64), ('Stern', 10.64), ('alternativ', 10.64)]\n",
      "[('Bohne', 100.0), ('Kaffee', 79.41), ('Aroma', 23.53), ('Geschmack', 23.53), ('Leck', 20.59), ('preis', 20.59), ('Qualität', 14.71), ('kaffeevollautomat', 14.71), ('vollautomat', 11.76), ('Packung', 11.76)]\n",
      "[('Stern', 100.0), ('Kaffee', 56.1), ('Geschmack', 17.07), ('preis', 14.63), ('Produkt', 12.2), ('Leck', 12.2), ('Aroma', 12.2), ('gut', 9.76), ('Bio', 9.76), ('lieblingskaffee', 9.76)]\n"
     ]
    }
   ],
   "source": [
    "bar = np.argsort([sum(mat[i]) for i in range(n_syn)])[::-1][:10]\n",
    "for k in bar:\n",
    "    foo = [(words[syn_i[i]], np.round(mat_p[k][i] * 100, decimals=2)) for i in np.argsort(mat_p[k])[::-1]][:10]\n",
    "    print(foo)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "80.0"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat[775][778]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "class ConditionalProbability:\n",
    "    def __init__(self, sentences, token_lists, syn, words):\n",
    "        self.sentences = sentences\n",
    "        self.token_lists = token_lists\n",
    "        self.synonym = syn\n",
    "        self.words = words\n",
    "        self.words_index = dict(zip(self.words, range(len(self.words))))\n",
    "        self.word_counts = words_counts\n",
    "        self.syn_token_list = self.syn_token_list()\n",
    "        self.token_sentence_map = self.token_sentence_map()\n",
    "\n",
    "    def syn_token_list(self):\n",
    "        '''\n",
    "        transforms the tokens in a token_list to the corresponding synonyms\n",
    "        :return:\n",
    "        '''\n",
    "        syn_token_list = []\n",
    "        for token_list in self.token_lists:\n",
    "            syn_tokens = np.unique([self.synonym[self.words_index[word]] for word in token_list]).tolist()\n",
    "            syn_token_list.append(syn_tokens)\n",
    "        return syn_token_list\n",
    "\n",
    "    def token_sentence_map(self):\n",
    "        '''\n",
    "        maps sentences to tokens, i.e. for each token we get a list of sentences which contain that token\n",
    "        :return:\n",
    "        '''\n",
    "        syn_token_to_sentence = defaultdict(list)\n",
    "        for i, syn_tokens in enumerate(self.syn_token_list):\n",
    "            for token in syn_tokens:\n",
    "                syn_token_to_sentence[token].append(i)\n",
    "        return syn_token_to_sentence\n",
    "\n",
    "    def sentences_from_tokens(self, tokens, literal=False):\n",
    "        '''\n",
    "        returns a list of sentences which contain all tokens specified in tokens\n",
    "        :param tokens:\n",
    "        :param literal:\n",
    "        :return:\n",
    "        '''\n",
    "        if literal:\n",
    "            tokens = [self.synonym[self.words_index[token]] for token in tokens]\n",
    "        return reduce(partial(np.intersect1d, assume_unique=True), [self.token_sentence_map[i] for i in tokens])\n",
    "\n",
    "    def cond_prob(self, event_token, condition_tokens, literal=False):\n",
    "        '''\n",
    "        returns the conditional probability of a sentence to contain event_token if that sentence\n",
    "        also contains all tokens specified in condition_tokens\n",
    "        :param event_token:\n",
    "        :param condition_tokens:\n",
    "        :param literal:\n",
    "        :return:\n",
    "        '''\n",
    "        if literal:\n",
    "            event_token = self.synonym[self.words_index[event_token]]\n",
    "        sen_with_token = self.sentences_from_tokens(condition_tokens, literal)\n",
    "        count = len(sen_with_token)\n",
    "        if count == 0:\n",
    "            return 0\n",
    "        sen_with_token = np.intersect1d(sen_with_token, self.token_sentence_map[event_token], assume_unique=True)\n",
    "        return len(sen_with_token) / count\n",
    "\n",
    "    def prob(self, event_token, literal=False):\n",
    "        if literal:\n",
    "            event_token = self.synonym[self.words_index[event_token]]\n",
    "        return len(self.token_sentence_map[event_token]) / len(self.syn_token_list)\n",
    "\n",
    "    def cond_prob_all_tokens(self, condition_tokens, return_probabilities=False, return_absolut=False, literal=False):\n",
    "        if literal:\n",
    "            condition_tokens = [self.synonym[self.words_index[token]] for token in condition_tokens]\n",
    "        sen_with_token = self.sentences_from_tokens(condition_tokens)\n",
    "        if len(sen_with_token) == 0:\n",
    "            return np.zeros(n_syn).tolist()\n",
    "        cond_probs = np.zeros(n_syn)\n",
    "        cond_abs = np.zeros(n_syn)\n",
    "        for i in range(n_syn):\n",
    "            if i not in condition_tokens:\n",
    "                all_events_count = len(np.intersect1d(sen_with_token, self.token_sentence_map[i], assume_unique=True))\n",
    "                cond_probs[i] =  all_events_count / len(sen_with_token)\n",
    "                cond_abs[i] = all_events_count\n",
    "            else:\n",
    "                cond_probs[i] = -1\n",
    "        mask = [True, return_probabilities, return_absolut]\n",
    "        result = np.array([np.argsort(cond_probs)[::-1], None, None])\n",
    "        if return_probabilities:\n",
    "            result[1] = np.sort(cond_probs)[::-1]\n",
    "        if return_absolut:\n",
    "            result[2] = cond_abs[result[0]]\n",
    "        return result[mask]\n",
    "\n",
    "cp = ConditionalProbability(sentences, token_lists, syn, words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, value, parent=None):\n",
    "        self.value = value\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "\n",
    "    def add_child(self, value):\n",
    "        node = Node(value, self)\n",
    "        self.children.append(node)\n",
    "        return self\n",
    "\n",
    "    def is_root(self):\n",
    "        return self.parent is None\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return len(self.children) == 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"%d\" % self.value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "    def name(self):\n",
    "        return str(self)\n",
    "\n",
    "    def cum_name(self):\n",
    "        node = self.parent\n",
    "        cum_name = [self.value]\n",
    "        while node is not None:\n",
    "            cum_name.insert(0, node.value)\n",
    "            node = node.parent\n",
    "        return cum_name\n",
    "\n",
    "    def get_leaves(self):\n",
    "        if self.is_leaf():\n",
    "            return [self]\n",
    "        leaves = []\n",
    "        for child in self.children:\n",
    "            leaves += child.get_leaves()\n",
    "        return leaves\n",
    "\n",
    "    def get_newick(self):\n",
    "        if self.is_leaf():\n",
    "            return self.name()\n",
    "        name_children = \",\".join([child.get_newick() for child in self.children])\n",
    "        return \"(%s)%s\" % (name_children, self.name())\n",
    "\n",
    "    def get_newick_alt(self, func):\n",
    "        if self.is_leaf():\n",
    "            return func(self.value)\n",
    "        name_children = \",\".join([child.get_newick_alt(func) for child in self.children])\n",
    "        return \"(%s)%s\" % (name_children, func(self.value))\n",
    "\n",
    "root = Node(778)\n",
    "threshold = 0.1\n",
    "threshold2 = 5\n",
    "threshold_rate = 0.05"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400, 62, 775, 62, 400, 775, 265, 62, 775, 764, 355, 344, 804, 561, 775, 804, 70]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def bar(root, threshold, threshold2):\n",
    "    leaves = root.get_leaves()\n",
    "    print(leaves)\n",
    "    for leaf in leaves:\n",
    "        a, b, c = cp.cond_prob_all_tokens(leaf.cum_name(), return_probabilities=True, return_absolut=True)\n",
    "        children = a[np.intersect1d(np.where(b > threshold)[0], np.where(c > threshold2)[0], assume_unique=True)]\n",
    "        print(children)\n",
    "        for child in children:\n",
    "            leaf.add_child(child)\n",
    "    return root\n",
    "\n",
    "bar(root, threshold, threshold2)\n",
    "threshold += threshold_rate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root.children[0].children[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "from ete3 import TreeStyle, Tree, TextFace, add_face_to_node\n",
    "t = Tree(root.get_newick_alt(lambda x: words[syn_i[x]]) + \";\" , format=1)\n",
    "\n",
    "ts = TreeStyle()\n",
    "ts.show_leaf_name = False\n",
    "ts.show_scale = False\n",
    "ts.show_branch_length = False\n",
    "def my_layout(node):\n",
    "    F = TextFace(node.name, tight_text=True)\n",
    "    add_face_to_node(F, node, column=0, position=\"branch-right\")\n",
    "ts.layout_fn = my_layout\n",
    "t.show(tree_style=ts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}