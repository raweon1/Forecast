{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "import queue\n",
    "import numpy as np\n",
    "import gensim\n",
    "from collections import defaultdict\n",
    "from functools import reduce, partial\n",
    "from scipy.spatial.distance import euclidean\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora, models\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from nltk import tokenize\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn import manifold\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from source.Topic.Utils import *\n",
    "from source.Topic.Model import *\n",
    "import re\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "meta = pd.read_csv(\"kaffee_reviews.csv\")\n",
    "meta.drop(columns=[\"index\"], inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for i in meta.review:\n",
    "    sentences.append(nltk.sent_tokenize(i, language='german'))\n",
    "sentences = np.concatenate(sentences).tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing raw texts ...\n",
      "Preprocessing raw texts. Done!\n"
     ]
    }
   ],
   "source": [
    "method = \"BERT_AE\" # \"LDA_BERT\"\n",
    "samp_size = len(meta.review)\n",
    "ntopic = 10\n",
    "\n",
    "rws = meta.review\n",
    "#rws = pd.Series(sentences)\n",
    "sentences, token_lists, idx_in = preprocess(sentences, samp_size=len(sentences))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "1191"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"distiluse-base-multilingual-cased\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "words, words_counts = np.unique(np.concatenate(token_lists), return_counts=True)\n",
    "embeddings = model.encode(words)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters: 45\n",
      "Estimated number of noise points: 762\n"
     ]
    }
   ],
   "source": [
    "def synonyme(unique_words_count, embeddings, eps=0.3, min_samples=2):\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples).fit(embeddings)\n",
    "    labels = db.labels_\n",
    "\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise_ = list(labels).count(-1)\n",
    "\n",
    "    print('Estimated number of clusters: %d' % n_clusters_)\n",
    "    print('Estimated number of noise points: %d' % n_noise_)\n",
    "    #[np.where(labels == i)[0] for i in range(n_clusters_)]\n",
    "    count = 0\n",
    "    syn = {}\n",
    "    syn_i = {}\n",
    "    for word_idx in np.where(labels == -1)[0]:\n",
    "        syn[word_idx] = count\n",
    "        syn_i[count] = word_idx\n",
    "        count += 1\n",
    "    for cluster in [np.where(labels == i)[0] for i in range(n_clusters_)]:\n",
    "        # print([words[j] for j in cluster])\n",
    "        for word_idx in cluster:\n",
    "            syn[word_idx] = count\n",
    "        syn_i[count] = cluster[unique_words_count[cluster].argmax()]\n",
    "        count += 1\n",
    "    return syn, syn_i, count\n",
    "\n",
    "syn, syn_i, n_syn = synonyme(words_counts, embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "mat = np.zeros((n_syn, n_syn))\n",
    "for k, token_list in enumerate(token_lists):\n",
    "    syn_tokens = np.unique([syn[np.where(words==i)[0][0]] for i in token_list])\n",
    "    token_combinations = np.array(np.meshgrid(syn_tokens, syn_tokens)).T.reshape(-1, 2)\n",
    "    for i, j in token_combinations:\n",
    "        if i == j:\n",
    "            mat[i][j] += 1 # len(syn_tokens) - 1\n",
    "        else:\n",
    "            mat[i][j] += 1\n",
    "\n",
    "mat_p = np.zeros((n_syn, n_syn))\n",
    "for i in range(n_syn):\n",
    "    if mat[i][i] != 0:\n",
    "        mat_p[i] = mat[i] / mat[i][i]\n",
    "        # mat_p[i][i] = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "[('Kaffee', 100.0),\n ('Geschmack', 10.32),\n ('Leck', 5.16),\n ('Aroma', 4.73),\n ('Bio', 3.66),\n ('Sorte', 3.44),\n ('Bohne', 3.44),\n ('Jahr', 3.23),\n ('gut', 2.58),\n ('Qualität', 2.58)]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kaffee = 239, syn[Kaffee] = 778\n",
    "# Geschmack = 178, syn[Geschmack] = 775\n",
    "# argmax 775\n",
    "# q = syn[np.where(words == \"Bewertung\")[0][0]]\n",
    "q = 778\n",
    "[(words[syn_i[i]], np.round(mat_p[q][i] * 100, decimals=2)) for i in np.argsort(mat_p[q])[::-1]][:10]\n",
    "# [np.round(sum(mat_p[i]), decimals=3) for i in range(mat_p.shape[0])]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "array([778, 774,  67, 763, 296,  78, 387, 456, 396, 407], dtype=int64)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort([sum(mat[i]) for i in range(n_syn)])[::-1][:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 775\n",
    "threshold = 10\n",
    "[(words[syn_i[i]], np.round(mat_p[:, p][i] * 100, decimals=2), mat[:,p][i]) for i in np.argsort(mat_p[:, p])[::-1] if mat[:, p][i] > threshold][:10]\n",
    "#mat_p[:, 775]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Kaffee', 100.0), ('Geschmack', 10.32), ('Leck', 5.16), ('Aroma', 4.73), ('Bio', 3.66), ('Sorte', 3.44), ('Bohne', 3.44), ('Jahr', 3.23), ('gut', 2.58), ('Qualität', 2.58)]\n",
      "[('Geschmack', 100.0), ('Kaffee', 36.09), ('Aroma', 9.77), ('Qualität', 5.26), ('Bohne', 4.51), ('Bio', 3.76), ('kaffeeautomat', 3.76), ('gut', 3.76), ('Sorte', 3.76), ('Duft', 3.01)]\n",
      "[('Bio', 100.0), ('Kaffee', 26.56), ('Geschmack', 7.81), ('gut', 6.25), ('Fairtrade', 6.25), ('Qualität', 6.25), ('Produkt', 4.69), ('fairtrade', 4.69), ('preis', 3.12), ('Sorte', 3.12)]\n",
      "[('Aroma', 100.0), ('Kaffee', 34.92), ('Geschmack', 20.63), ('Bohne', 4.76), ('Produkt', 4.76), ('Gefühl', 3.17), ('Röstung', 3.17), ('Bio', 3.17), ('bisschen', 3.17), ('bohn', 3.17)]\n",
      "[('Leck', 100.0), ('Kaffee', 43.64), ('Geschmack', 7.27), ('Latte', 5.45), ('gewiß', 5.45), ('Bohne', 5.45), ('Firma', 3.64), ('Versand', 3.64), ('Original', 3.64), ('Milch', 3.64)]\n",
      "[('Bohne', 100.0), ('Kaffee', 41.03), ('Geschmack', 15.38), ('kaffeeautomat', 10.26), ('Aroma', 7.69), ('Leck', 7.69), ('Zeit', 5.13), ('Packung', 5.13), ('vollautomat', 5.13), ('Temperatur', 5.13)]\n",
      "[('Preis', 100.0), ('Kaffee', 18.75), ('Ordnung', 10.42), ('Qualität', 8.33), ('Leistungsverhältnis', 6.25), ('Kilo', 4.17), ('versanden', 4.17), ('Bio', 4.17), ('Geschmack', 4.17), ('Stern', 4.17)]\n",
      "[('Sorte', 100.0), ('Kaffee', 36.36), ('Geschmack', 11.36), ('vollautomat', 4.55), ('Magen', 4.55), ('Favorit', 4.55), ('Jahr', 4.55), ('reichen', 4.55), ('Bio', 4.55), ('Qualität', 4.55)]\n",
      "[('Produkt', 100.0), ('Kaffee', 31.43), ('Aroma', 8.57), ('Stern', 8.57), ('Geschmack', 8.57), ('Bio', 8.57), ('Mehrwert', 5.71), ('Aspekt', 5.71), ('Tee', 5.71), ('Sinn', 5.71)]\n",
      "[('Qualität', 100.0), ('Kaffee', 37.5), ('Geschmack', 21.88), ('Preis', 12.5), ('Bio', 12.5), ('Jahr', 9.38), ('Verpackung', 6.25), ('versanden', 6.25), ('gut', 6.25), ('Sorte', 6.25)]\n"
     ]
    }
   ],
   "source": [
    "bar = np.argsort([sum(mat[i]) for i in range(n_syn)])[::-1][:10]\n",
    "for k in bar:\n",
    "    foo = [(words[syn_i[i]], np.round(mat_p[k][i] * 100, decimals=2)) for i in np.argsort(mat_p[k])[::-1]][:10]\n",
    "    print(foo)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "80.0"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat[775][778]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class ConditionalProbability:\n",
    "    def __init__(self, sentences, token_lists, syn, words):\n",
    "        self.sentences = sentences\n",
    "        self.token_lists = token_lists\n",
    "        self.synonym = syn\n",
    "        self.words = words\n",
    "        self.words_index = dict(zip(self.words, range(len(self.words))))\n",
    "        self.word_counts = words_counts\n",
    "        self.syn_token_list = self.syn_token_list()\n",
    "        self.token_sentence_map = self.token_sentence_map()\n",
    "\n",
    "    def syn_token_list(self):\n",
    "        '''\n",
    "        transforms the tokens in a token_list to the corresponding synonyms\n",
    "        :return:\n",
    "        '''\n",
    "        syn_token_list = []\n",
    "        for token_list in self.token_lists:\n",
    "            syn_tokens = np.unique([self.synonym[self.words_index[word]] for word in token_list]).tolist()\n",
    "            syn_token_list.append(syn_tokens)\n",
    "        return syn_token_list\n",
    "\n",
    "    def token_sentence_map(self):\n",
    "        '''\n",
    "        maps sentences to tokens, i.e. for each token we get a list of sentences which contain that token\n",
    "        :return:\n",
    "        '''\n",
    "        syn_token_to_sentence = defaultdict(list)\n",
    "        for i, syn_tokens in enumerate(self.syn_token_list):\n",
    "            for token in syn_tokens:\n",
    "                syn_token_to_sentence[token].append(i)\n",
    "        return syn_token_to_sentence\n",
    "\n",
    "    def sentences_from_tokens(self, tokens, literal=False):\n",
    "        '''\n",
    "        returns a list of sentences which contain all tokens specified in tokens\n",
    "        :param tokens:\n",
    "        :param literal:\n",
    "        :return:\n",
    "        '''\n",
    "        if literal:\n",
    "            tokens = [self.synonym[self.words_index[token]] for token in tokens]\n",
    "        return reduce(partial(np.intersect1d, assume_unique=True), [self.token_sentence_map[i] for i in tokens])\n",
    "\n",
    "    def cond_prob(self, event_token, condition_tokens, literal=False):\n",
    "        '''\n",
    "        returns the conditional probability of a sentence to contain event_token if that sentence\n",
    "        also contains all tokens specified in condition_tokens\n",
    "        :param event_token:\n",
    "        :param condition_tokens:\n",
    "        :param literal:\n",
    "        :return:\n",
    "        '''\n",
    "        if literal:\n",
    "            event_token = self.synonym[self.words_index[event_token]]\n",
    "        sen_with_token = self.sentences_from_tokens(condition_tokens, literal)\n",
    "        count = len(sen_with_token)\n",
    "        if count == 0:\n",
    "            return 0\n",
    "        sen_with_token = np.intersect1d(sen_with_token, self.token_sentence_map[event_token], assume_unique=True)\n",
    "        return len(sen_with_token) / count\n",
    "\n",
    "    def prob(self, event_token, literal=False):\n",
    "        if literal:\n",
    "            event_token = self.synonym[self.words_index[event_token]]\n",
    "        return len(self.token_sentence_map[event_token]) / len(self.syn_token_list)\n",
    "\n",
    "    def cond_prob_all_tokens(self, condition_tokens, return_probabilities=False, return_absolut=False, literal=False):\n",
    "        if literal:\n",
    "            condition_tokens = [self.synonym[self.words_index[token]] for token in condition_tokens]\n",
    "        sen_with_token = self.sentences_from_tokens(condition_tokens)\n",
    "        if len(sen_with_token) == 0:\n",
    "            return np.zeros(n_syn).tolist()\n",
    "        cond_probs = np.zeros(n_syn)\n",
    "        cond_abs = np.zeros(n_syn)\n",
    "        for i in range(n_syn):\n",
    "            if i not in condition_tokens:\n",
    "                all_events_count = len(np.intersect1d(sen_with_token, self.token_sentence_map[i], assume_unique=True))\n",
    "                cond_probs[i] =  all_events_count / len(sen_with_token)\n",
    "                cond_abs[i] = all_events_count\n",
    "            else:\n",
    "                cond_probs[i] = -1\n",
    "        mask = [True, return_probabilities, return_absolut]\n",
    "        result = np.array([np.argsort(cond_probs)[::-1], None, None])\n",
    "        if return_probabilities:\n",
    "            result[1] = np.sort(cond_probs)[::-1]\n",
    "        if return_absolut:\n",
    "            result[2] = cond_abs[result[0]]\n",
    "        return result[mask]\n",
    "\n",
    "cp = ConditionalProbability(sentences, token_lists, syn, words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, value, parent=None):\n",
    "        self.value = value\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "\n",
    "    def add_child(self, value):\n",
    "        node = Node(value, self)\n",
    "        self.children.append(node)\n",
    "        return self\n",
    "\n",
    "    def is_root(self):\n",
    "        return self.parent is None\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return len(self.children) == 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"%d\" % self.value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "    def name(self):\n",
    "        return str(self)\n",
    "\n",
    "    def cum_name(self):\n",
    "        node = self.parent\n",
    "        cum_name = [self.value]\n",
    "        while node is not None:\n",
    "            cum_name.insert(0, node.value)\n",
    "            node = node.parent\n",
    "        return cum_name\n",
    "\n",
    "    def get_leaves(self):\n",
    "        if self.is_leaf():\n",
    "            return [self]\n",
    "        leaves = []\n",
    "        for child in self.children:\n",
    "            leaves += child.get_leaves()\n",
    "        return leaves\n",
    "\n",
    "    def get_newick(self):\n",
    "        if self.is_leaf():\n",
    "            return self.name()\n",
    "        name_children = \",\".join([child.get_newick() for child in self.children])\n",
    "        return \"(%s)%s\" % (name_children, self.name())\n",
    "\n",
    "    def get_newick_alt(self, func):\n",
    "        if self.is_leaf():\n",
    "            return func(self.value)\n",
    "        name_children = \",\".join([child.get_newick_alt(func) for child in self.children])\n",
    "        return \"(%s)%s\" % (name_children, func(self.value))\n",
    "\n",
    "root = Node(778)\n",
    "threshold = 0.03\n",
    "threshold2 = 2\n",
    "threshold_rate = 0.00"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def bar(root, threshold, threshold2):\n",
    "    new_child = False\n",
    "    leaves = root.get_leaves()\n",
    "    for leaf in leaves:\n",
    "        a, b, c = cp.cond_prob_all_tokens(leaf.cum_name(), return_probabilities=True, return_absolut=True)\n",
    "        children = a[np.intersect1d(np.where(b > threshold)[0], np.where(c > threshold2)[0], assume_unique=True)]\n",
    "        print(children)\n",
    "        for child in children:\n",
    "            leaf.add_child(child)\n",
    "            new_child = True\n",
    "    return new_child\n",
    "\n",
    "bar(root, threshold, threshold2)\n",
    "threshold += threshold_rate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root.children[0].children[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from ete3 import TreeStyle, Tree, TextFace, add_face_to_node\n",
    "t = Tree(root.get_newick_alt(lambda x: words[syn_i[x]]) + \";\" , format=1)\n",
    "\n",
    "ts = TreeStyle()\n",
    "ts.show_leaf_name = False\n",
    "ts.show_scale = False\n",
    "ts.show_branch_length = False\n",
    "def my_layout(node):\n",
    "    F = TextFace(node.name, tight_text=True)\n",
    "    add_face_to_node(F, node, column=0, position=\"branch-right\")\n",
    "ts.layout_fn = my_layout\n",
    "t.show(tree_style=ts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([ 74.,   0., 114.,   0., 105.,   0.,  55.,   0.,  30.,   0.,   0.,\n          9.,   0.,   9.,   0.,   5.,   0.,   2.,   0.,   2.]),\n array([ 1.  ,  1.45,  1.9 ,  2.35,  2.8 ,  3.25,  3.7 ,  4.15,  4.6 ,\n         5.05,  5.5 ,  5.95,  6.4 ,  6.85,  7.3 ,  7.75,  8.2 ,  8.65,\n         9.1 ,  9.55, 10.  ]),\n <a list of 20 Patch objects>)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(tokens) for tokens in token_lists if \"Kaffee\" in tokens], bins=20)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}